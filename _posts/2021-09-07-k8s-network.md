---
title: k8s网络模型说明(转自阿里云公开课)
tags: TeXt
article_header:
  type: cover
---

### 早期的docker网络
     容器网络发端于 Docker 的网络。Docker 使用了一个比较简单的网络模型，即内部的网桥加内部的保留 IP。
     这种设计的好处在于容器的网络和外部世界是解耦的，无需占用宿主机的 IP 或者宿主机的资源，完全是虚拟的。
     它的设计初衷是：当需要访问外部世界时，会采用 SNAT 这种方法来借用 Node 的 IP 去访问外面的服务。
     比如容器需要对外提供服务的时候，所用的是 DNAT 技术，也就是在 Node 上开一个端口，然后通过 iptable 或者别的某些机制，把流导入到容器的进程上以达到目的。
- 基于私网IP的网桥模式
  - 网桥(docker0)、veth-pair(172.x.x.x.x)
#####  优势:
     - 和外部解耦、使用私有网络和网桥、网段充裕
     - 出宿主机采用SNAT借IP、进入宿主机采用DNAT借端口
##### 问题
    该模型的问题在于，外部网络无法区分哪些是容器的网络与流量、哪些是宿主机的网络与流量。
    比如，如果要做一个高可用的时候，172.16.1.1 和 172.16.1.2 是拥有同样功能的两个容器，
    此时我们需要将两者绑成一个 Group 对外提供服务，而这个时候我们发现从外部看来两者没有相同之处，
    它们的 IP 都是借用宿主机的端口，因此很难将两者归拢到一起。

 - 多个容器之间很多NAT之间相互访问无法直接标识和识别
![k8s docker net](/assets/images/posts/k8_docker_net.png)

#### Kubernetes网络模型
    在此基础上，Kubernetes 提出了这样一种机制：即每一个 Pod，也就是一个功能聚集小团伙应有自己的“身份证”，或者说 ID。
    在 TCP 协议栈上，这个 ID 就是 IP。 这个 IP 是真正属于该 Pod 的，外部世界不管通过什么方法一定要给它。
    对这个 Pod IP 的访问就是真正对它的服务的访问，中间拒绝任何的变造。
    比如以 10.1.1.1 的 IP 去访问 10.1.2.1 的 Pod，结果到了 10.1.2.1 上发现，它实际上借用的是宿主机的 IP，而不是源 IP，
    这样是不被允许的。Pod 内部会要求共享这个 IP，从而解决了一些功能内聚的容器如何变成一个部署的原子的问题。
      剩下的问题是我们的部署手段。Kubernetes 对怎么实现这个模型其实是没有什么限制的，
     用 underlay 网络来控制外部路由器进行导流是可以的；如果希望解耦，用 overlay 网络在底层网络之上再加一层叠加网，
     这样也是可以的。总之，只要达到模型所要求的目的即可。

### Kubernetes 对网络没有具体的实现约束你可以实现自己的插件CNI
- 约法三章
  - 任意两个 pod 之间其实是可以直接通信的，无需经过显式地使用 NAT 来接收数据和地址的转换；
  - node 与 pod 之间是可以直接通信的，无需使用明显的地址转换；
  - pod 看到自己的 IP 跟别人看见它所用的IP是一样的，中间不能经过转换。
- 四大目标
  - 外部世界和 service 之间是怎么通信的？就是有一个互联网或者是公司外部的一个用户，怎么用到 service？service 特指 K8s 里面的服务概念。
  - service 如何与它后端的 pod 通讯？
  - pod 和 pod 之间调用是怎么做到通信的？
  - 最后就是 pod 内部容器与容器之间的通信？

### 网络拓扑
- 唯一标识 自己视图和外部试图一致性 全球唯一性 无显示NAT
- pod内部共享这个标识
- 不限制具体实现
![网络](/assets/images/posts/k8_net.png)

### pod如何上网
- 协议维度 L2(mac) --> L3(IP) --> L4 (协议+端口)
- 网络拓扑 容器 --> pod  --> node --> remote
###### 具体过程
 - 容器送出网络包L2(mac): infro最先启动将c1和c2加入自己的network namespace中共享一个IP,容器C1和infra之间二层网络通过mac通讯
 - 容器到宿主机L3(IP):
    - 实现方式一： 经典方式 :Veth + bridge、Veth + pair
    - 实现方式二： 高版本内核的新机制等其他方式 mac/IPvlan
 - 宿主机到远端L4(协议加端口):
   - 实现方式一：underlay 容器和宿主机位于同一层网络 完全二层网络
   - 实现方式二：overlay  容器独立于主机的IP段 跨主机网络通信时是通过在主机之间创建隧道的方式，将整个容器网段的包全都封装成底层的物理网络中主机之间的包。不依赖底层网络(L2) 隧道技术 vxlan
   - 实现方式三：路由  主机和容器也分属不同的网段 跨主机通信是通过路由打通 靠路由打通部门依赖底层 host--> host
  ###### L4实现中的环境限制:
   #### 虚拟机vm :
         不允许机器之间直接通过二层协议访问，必须要带有IP地址这种三层的才能去做转发，限制某一个机器只能使用某些IP等。
         在这种被做了强限制的底层网络中，只能去选择 Overlay 的插件，
         常见的有 Flannel-vxlan, Calico-ipip, Weave 等等；
   #### 物理机环境
         限制少 同一个交换机下面直接做一个二层的通信。选择 Underlay或者路由模式的插件。
         Underlay意味着我们可以直接在一个物理机上插多个网卡或者是在一些网卡上做硬件虚拟化；
         路由模式就是依赖于 Linux 的路由协议做一个打通。这样就避免了像 vxlan 的封包方式导致的性能降低。
         这种环境下我们可选的插件包括 clico-bgp, flannel-hostgw, sriov 等等；
   #### 公有云环境也是虚拟化，
        因此底层限制也会较多。但每个公有云都会考虑适配容器，提升容器的性能，
        因此每家公有云可能都提供了一些 API 去配置一些额外的网卡或者路由这种能力。
        在公有云上，我们要尽量选择公有云厂商提供的 CNI 插件以达到兼容性和性能上的最优。
![pod网络](/assets/images/posts/pod_net.png)

  #### 最简单的路由方案：Flannel-host-gw
        这个方案采用的是每个Node独占网段，每个Subnet会绑定在一个Node上，网关也设置在本地，
      或者说直接设在cni0这个网桥的内部端口上。该方案的好处是管理简单，坏处就是无法跨Node迁移 Pod。
      就是说这个 IP、网段已经是属于这个Node之后就无法迁移到别的Node上。

![Flannel-host-gw](/assets/images/posts/Flannel-host-gw.png)
######## 这个方案的核心在于route表的设置
- 第一条很简单，我们在设置网卡的时候都会加上这一行。就是指定我的默认路由是通过哪个 IP 走掉，默认设备又是什么；
- 第二条是对 Subnet 的一个规则反馈。就是说我的这个网段是 10.244.0.0，掩码是 24 位，它的网关地址就在网桥上，也就是 10.244.0.1。
  这就是说这个网段的每一个包都发到这个网桥的 IP 上；
- 第三条是对对端的一个反馈。如果你的网段是 10.244.1.0（上图右边的 Subnet），
  我们就把它的 Host 的网卡上的 IP (10.168.0.3) 作为网关。也就是说，如果数据包是往 10.244.1.0 这个网段发的，
  就请以 10.168.0.3 作为网关。

######## 具体流程

    假设容器 (10.244.0.2) 想要发一个包给 10.244.1.3，那么它在本地产生了 TCP 或者 UDP 包之后，
    再依次填好对端 IP 地址、本地以太网的 MAC 地址作为源 MAC 以及对端 MAC。
    一般来说本地会设定一条默认路由，默认路由会把 cni0 上的 IP 作为它的默认网关，对端的 MAC 就是这个网关的 MAC 地址。
    然后这个包就可以发到桥上去了。如果网段在本桥上，那么通过 MAC 层的交换即可解决。
    这个例子中我们的 IP 并不属于本网段，因此网桥会将其上送到主机的协议栈去处理。主机协议栈恰好找到了对端的 MAC 地址。
    使用 10.168.0.3 作为它的网关，通过本地 ARP 探查后，我们得到了 10.168.0.3 的 MAC 地址。
    即通过协议栈层层组装，我们达到了目的，将 Dst-MAC 填为右图主机网卡的 MAC 地址，从而将包从主机的 eth0 发到对端的 eth0 上去。

         可以发现，这里有一个隐含的限制，上图中的 MAC 地址填好之后一定是能到达对端的，但如果这两个宿主机之间不是二层连接的，
    中间经过了一些网关、一些复杂的路由，那么这个 MAC 就不能直达，这种方案就是不能用的。当包到达了对端的 MAC 地址之后，
    发现这个包确实是给它的，但是 IP 又不是它自己的，就开始 Forward 流程，包上送到协议栈，之后再走一遍路由，
    刚好会发现 10.244.1.0/24 需要发到 10.244.1.1 这个网关上，从而到达了 cni0 网桥，它会找到 10.244.1.3 对应的 MAC 地址，
    再通过桥接机制，这个包就到达了对端容器。可以看到，整个过程总是二层、三层，发的时候又变成二层，再做路由，就是一个大环套小环。
    这是一个比较简单的方案，如果中间要走隧道，则可能会有一条 vxlan tunnel 的设备，此时就不填直接的路由，而填成对端的隧道号。

### Service 上网
- service 其实是一种用户侧负载均衡 (Load Balance) 的机制。
![img_1.png](/assets/images/posts/svc_net.png)
- 首先是由一群 Pod 组成一组功能后端，再在前端上定义一个虚 IP 作为访问入口。一般来说，由于 IP 不太好记，我们还会附赠一个 DNS 的域名，
- Client 先访问域名得到虚 IP 之后再转成实 IP。
- Kube-proxy 则是整个机制的实现核心，它隐藏了大量的复杂性。它的工作机制是通过 apiserver 监控 Pod/Service 的变化（比如是不是新增了 Service、Pod）
  并将其反馈到本地的规则或者是用户态进程

### 一个LVS版的Service (专门用于负载均衡的内核机制 它工作在第四层，性能会比用 iptable 实现好一些)
    假设我们是一个Kube-proxy，拿到了一个 Service的配置，如下图所示：
    它有一个 Cluster IP，在该 IP 上的端口是 9376，需要反馈到容器上的是 80 端口，
    还有三个可工作的 Pod，它们的 IP 分别是 10.1.2.3, 10.1.14.5, 10.1.3.8。

![lvs net](/assets/images/posts/lvs_k8.png)
- 第 1 步，绑定 VIP 到本地（欺骗内核)，
   - 首先需要让内核相信它拥有这样的一个虚 IP，
  这是 LVS 的工作机制所决定的，因为它工作在第四层，并不关心 IP 转发，
  只有它认为这个 IP 是自己的才会拆到 TCP 或 UDP 这一层。
  在第一步中，我们将该 IP 设到内核中，告诉内核它确实有这么一个 IP。
  实现的方法有很多，我们这里用的是 ip route 直接加 local 的方式，用 Dummy 哑设备上加 IP 的方式也是可以的。
   ```shell
     ip route add to local 192.168.60.200/32 dev eth0 proto kernel
   ```

- 第 2 步，为这个虚 IP 创建一个 IPVS 的 virtual server；
  - 告诉它我需要为这个 IP 进行负载均衡分发，
    后面的参数就是一些分发策略等等。virtual server 的 IP 其实就是我们的 Cluster IP。
   ```shell
     ipvsadm -A -t 192.168.60.200:9376 -s rr -p 600
   ```
- 第 3 步，为这个 IPVS service 创建相应的 real server
  - 需要为 virtual server 配置相应的 real server，
    就是真正提供服务的后端是什么。比如说我们刚才看到有三个 Pod，于是就把这三个的 IP 配到 virtual server 上，
    完全一一对应过来就可以了。Kube-proxy 工作跟这个也是类似的。只是它还需要去监控一些 Pod 的变化，
    比如 Pod 的数量变成 5 个了，那么规则就应变成 5 条。如果这里面某一个 Pod 死掉了或者被杀死了，那么就要相应地减掉一条。
    又或者整个 Service 被撤销了，那么这些规则就要全部删掉。所以它其实做的是一些管理层面的工作。
 ```shell
   ipvsadm -a -t 192.168.60.200:9376 10.1.2.3:80 -r -m
   ipvsadm -a -t 192.168.60.200:9376 10.1.4.5:80 -r -m
   ipvsadm -a -t 192.168.60.200:9376 10.1.3.8:80 -r -m
 ```

### 外部负载均衡
 - 集群内部负载均衡
    - 默认的内部模式 ClusterIP 是一个虚拟IP绑定到一组服务的Group Pod
 - 集群外部部负载均衡
    - 本地外部模式： Service 绑定到Node的静态端口,与Service一一对应,集群外通过 <NodeIP>:<NodePort>的方式访问Service。
 - LoadBalancer
    - 云厂商的扩展接口 首先会自动创建 NodePort 和 ClusterIP 这两种机制
       - 方案一 : 云厂商可以选择直接将 LB 挂到这两种机制上
       - 方案二 : Pod 的 RIP 挂到云厂商的 ELB 的后端
 - ExternalName
   - 完全自我实现 依赖外部设施 此时一个 Service 会和一个域名一一对应起来，整个负载均衡的工作都是外部实现的
##### 经典内部和外部结合的架构
    灵活地应用了 ClusterIP、NodePort 等多种服务方式，
    又结合了云厂商的 ELB，变成了一个很灵活、极度伸缩、生产上真正可用的一套系统
    首先我们用 ClusterIP 来做功能 Pod 的服务入口。大家可以看到，如果有三种 Pod 的话，就有三个 Service Cluster IP 作为它们的服务入口。这些方式都是 Client 端的，如何在 Server 端做一些控制呢？

- 容器内部负载均衡(任何一个环节都不存在单点的问题，任何一个环节也都有管理与反馈。)
  - 启动Ingress（Ingress 是 K8s 后来新增的一种服务，本质上还是一堆同质的 Pod），Ingress提供对外入口
  - 任何一个请求访问23456端口的Pod, 就会访问到 Ingress 的服务
  - Ingress服务的Controller把Service IP 和 Ingress 的后端进行映射关联，最后会调到 ClusterIP，再调到我们的功能 Pod。
  - 访问云厂商的 ELB，ELB 去监听所有集群节点上的 23456 端口，只要在 23456 端口上有服务的，就有一个 Ingress 的实例在跑。
  - 整个的流量经过外部域名的一个解析跟分流到达了云厂商的 ELB，ELB 经过负载均衡并通过 NodePort 的方式到达 Ingress，

### LoadBalancer ClusterIP、NodePort
![lb net](/assets/images/posts/lb_svc_net.png)
